# Artistry Technical Documentation

**Complete Technical Reference for Developers**

Version: 2.1 (Pipeline Update)  
Last Updated: December 20, 2025

> **âš ï¸ Important Update:** This documentation reflects the major pipeline redesign implementing img2img with ControlNet for real interior redesigns. See [PIPELINE_UPDATE.md](./PIPELINE_UPDATE.md) for migration details.

---

## ğŸ“‹ Table of Contents

1. [Executive Summary](#executive-summary)
2. [System Overview](#system-overview)
3. [Architecture Deep Dive](#architecture-deep-dive)
4. [Technology Stack](#technology-stack)
5. [Service Specifications](#service-specifications)
6. [API Reference](#api-reference)
7. [Database Design](#database-design)
8. [Security & Authentication](#security--authentication)
9. [Performance & Optimization](#performance--optimization)
10. [Deployment](#deployment)
11. [Development Guide](#development-guide)
12. [Testing Strategy](#testing-strategy)
13. [Troubleshooting](#troubleshooting)
14. [Future Enhancements](#future-enhancements)

---

## Executive Summary

### What is Artistry?

Artistry is a production-ready, microservices-based AI platform for interior design transformation. It leverages state-of-the-art computer vision and generative AI models to analyze room photos and produce professional design visualizations.

### Key Technical Features

- **Microservices Architecture**: 5 independent services (Gateway + 4 AI services)
- **Modern Stack**: FastAPI backends, React 18 + Vite frontend
- **AI Models**: YOLOv8, MobileSAM, Vision-LLM, Stable Diffusion v1.5
- **Database**: MongoDB Atlas (cloud-hosted NoSQL)
- **Scalability**: Horizontally scalable, containerizable services
- **Performance**: 30-60s full workflow on CPU, 12-15s with GPU

### Business Value

- **Cost-Effective**: Runs on CPU, GPU optional for speed
- **Scalable**: Independent service scaling based on demand
- **Maintainable**: Isolated services, clear separation of concerns
- **Extensible**: Easy to add new AI models/services
- **Production-Ready**: Error handling, logging, health monitoring

---

## System Overview

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Client Layer (Browser)                    â”‚
â”‚                  React 18 + Vite + Tailwind                  â”‚
â”‚                                                              â”‚
â”‚  User Interface Components:                                  â”‚
â”‚  â€¢ Image Upload & Preview                                    â”‚
â”‚  â€¢ Progress Tracking (Step Indicators)                       â”‚
â”‚  â€¢ Results Visualization (Grid Layout)                       â”‚
â”‚  â€¢ Navigation & Routing                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTP/REST API (JSON)
                       â”‚ CORS-enabled, async fetch
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   API Gateway Layer                          â”‚
â”‚              FastAPI + Motor + MongoDB Atlas                 â”‚
â”‚                     Port 8000                                â”‚
â”‚                                                              â”‚
â”‚  Responsibilities:                                           â”‚
â”‚  â€¢ Request routing & orchestration                           â”‚
â”‚  â€¢ Data persistence (future)                                 â”‚
â”‚  â€¢ Health monitoring                                         â”‚
â”‚  â€¢ Cross-service communication                               â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”˜
    â”‚          â”‚          â”‚          â”‚                     â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚ Detect â”‚ â”‚Segment â”‚ â”‚ Advise â”‚ â”‚  Generate  â”‚          â”‚
â”‚  AI    â”‚ â”‚   AI   â”‚ â”‚   AI   â”‚ â”‚     AI     â”‚          â”‚
â”‚ :8001  â”‚ â”‚ :8002  â”‚ â”‚ :8003  â”‚ â”‚   :8004    â”‚          â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚
    â”‚          â”‚          â”‚             â”‚                 â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚YOLOv8n â”‚ â”‚Mobile  â”‚ â”‚GPT-2 / â”‚ â”‚   Stable   â”‚          â”‚
â”‚ Model  â”‚ â”‚  SAM   â”‚ â”‚LLaVA-  â”‚ â”‚ Diffusion  â”‚          â”‚
â”‚(6 MB)  â”‚ â”‚(10 MB) â”‚ â”‚inspiredâ”‚ â”‚ v1.5 +     â”‚          â”‚
â”‚        â”‚ â”‚        â”‚ â”‚(500 MB)â”‚ â”‚ ControlNet â”‚          â”‚
â”‚        â”‚ â”‚        â”‚ â”‚        â”‚ â”‚   (5 GB)   â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                                                           â”‚
                               MongoDB Atlas â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          (Shared Database - Cloud)
```

### Request Flow (Full Workflow)

```
User Action â†’ Frontend Upload â†’ API Service Layer
                    â†“
         [Sequential Processing]
                    â†“
    1. POST /detect/  â†’ Object Detection (2-3s)
                    â†“
    2. POST /segment/ â†’ Image Segmentation (3-5s)
                    â†“
    3. POST /advise/  â†’ Design Advice (5-8s)
                    â†“
    4. POST /generate/ â†’ AI Generation (30-40s)
                    â†“
    [Optional] POST /designs/ â†’ Save to MongoDB
                    â†“
         Display Unified Results
```

---

## Architecture Deep Dive

### Design Patterns

#### 1. Microservices Pattern
- **Isolation**: Each service runs independently
- **Single Responsibility**: One AI model per service
- **Independent Deployment**: Deploy/update without affecting others
- **Technology Heterogeneity**: Different frameworks per service if needed

#### 2. API Gateway Pattern
- **Single Entry Point**: Gateway service at port 8000
- **Request Routing**: Directs traffic to appropriate services
- **Cross-Cutting Concerns**: Authentication, logging, monitoring

#### 3. Service-Oriented Architecture (SOA)
- **Loose Coupling**: Services communicate via REST API
- **Contract-Based**: Clear API contracts (OpenAPI/Swagger)
- **Stateless**: Each request is independent

### Communication Protocols

**Current Implementation**: Synchronous REST
```
Client â†’ Gateway â†’ Service â†’ Response
         (HTTP)    (HTTP)
```

**Planned Enhancement**: Asynchronous Message Queue
```
Client â†’ Gateway â†’ Queue (RabbitMQ/Redis)
                     â†“
              [Worker Pool]
                     â†“
         Results â†’ MongoDB â†’ WebSocket â†’ Client
```

### Data Flow Patterns

**Upload Flow**:
```javascript
// Frontend
const formData = new FormData();
formData.append('file', imageFile);

fetch('http://localhost:8001/detect/', {
  method: 'POST',
  body: formData
})
```

**Response Flow**:
```json
{
  "status": "success",
  "data": {
    "objects": [...],
    "annotated_image": "base64..."
  },
  "metadata": {
    "processing_time": 2.3,
    "model_version": "yolov8n"
  }
}
```

---

## Technology Stack

### Frontend Technologies

| Technology | Version | Purpose | Justification |
|------------|---------|---------|---------------|
| **React** | 18.3.1 | UI Framework | Virtual DOM, component reusability, large ecosystem |
| **Vite** | 5.4.10 | Build Tool | Faster than Webpack, HMR, modern ES modules |
| **React Router** | 6.28.0 | Client Routing | SPA navigation, nested routes |
| **Tailwind CSS** | 3.4.14 | Styling | Utility-first, responsive, small bundle |
| **Lucide React** | 0.454.0 | Icons | Tree-shakeable, consistent design |

**Build Configuration** (vite.config.js):
```javascript
export default defineConfig({
  plugins: [react()],
  server: {
    port: 5173,
    proxy: {
      '/api': 'http://localhost:8000'
    }
  },
  build: {
    outDir: 'dist',
    sourcemap: true,
    minify: 'terser'
  }
})
```

### Backend Technologies

| Technology | Version | Purpose | Justification |
|------------|---------|---------|---------------|
| **FastAPI** | 0.115.5 | Web Framework | Async support, auto docs, type validation |
| **Uvicorn** | 0.32.1 | ASGI Server | High performance, WebSocket support |
| **Motor** | 3.6.0 | MongoDB Driver | Async MongoDB operations |
| **Pydantic** | 2.x | Data Validation | Type safety, JSON schema generation |

### AI/ML Stack

| Component | Technology | Model Size | Purpose |
|-----------|-----------|------------|---------|
| **Object Detection** | Ultralytics YOLOv8n | 6 MB | Real-time object detection |
| **Segmentation** | MobileSAM | 10 MB | Efficient image segmentation |
| **NLP/Vision** | Transformers + GPT-2 | 500 MB | Design advice generation |
| **Image Generation** | Diffusers + SD v1.5 | 5 GB | Photorealistic generation |
| **Deep Learning** | PyTorch 2.4.0 | - | Model inference framework |

**PyTorch Configuration**:
```python
import torch

# CPU optimization
torch.set_num_threads(4)
torch.set_grad_enabled(False)  # Inference only

# GPU configuration (if available)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device).eval()

# Mixed precision for speed
from torch.cuda.amp import autocast
with autocast():
    output = model(input)
```

### Database

**MongoDB Atlas**:
- **Type**: Cloud-hosted NoSQL
- **Tier**: Free (M0) - 512MB storage
- **Driver**: Motor (async Python driver)
- **Connection Pooling**: Min 10, Max 50 connections

**Connection String**:
```python
MONGO_URI = "mongodb+srv://user:pass@cluster.mongodb.net/artistry?retryWrites=true&w=majority"

client = AsyncIOMotorClient(
    MONGO_URI,
    maxPoolSize=50,
    minPoolSize=10,
    serverSelectionTimeoutMS=5000
)
```

---

## Service Specifications

### 1. Gateway Service

**Technical Specs**:
- **Framework**: FastAPI 0.115
- **Port**: 8000
- **Language**: Python 3.10
- **Dependencies**: Motor, HTTPx, Pydantic
- **Memory**: ~50 MB idle, ~200 MB active
- **CPU**: <5% idle, <20% active

**Implementation**:
```python
from fastapi import FastAPI, HTTPException
from motor.motor_asyncio import AsyncIOMotorClient
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI(
    title="Artistry Gateway",
    version="2.0",
    docs_url="/docs"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

client = AsyncIOMotorClient(MONGO_URI)
db = client.artistry

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "service": "gateway",
        "version": "2.0",
        "database": "connected" if await db.command("ping") else "disconnected"
    }
```

### 2. Detect Service

**Technical Specs**:
- **Framework**: FastAPI + Ultralytics
- **Port**: 8001
- **Model**: YOLOv8n (3.2M parameters)
- **Memory**: ~800 MB
- **CPU Usage**: 40-60% during inference
- **Response Time**: 1-3s (CPU), 0.3-0.5s (GPU)

**Model Loading**:
```python
from ultralytics import YOLO
import cv2
import numpy as np

# Load model (once at startup)
model = YOLO("app/yolov8n.pt")
model.fuse()  # Optimize for inference

@app.post("/detect/")
async def detect_objects(file: UploadFile):
    # Read image
    contents = await file.read()
    nparr = np.frombuffer(contents, np.uint8)
    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    
    # Run inference
    results = model(image, conf=0.25)
    
    # Extract objects
    objects = []
    for box in results[0].boxes:
        objects.append({
            "class": results[0].names[int(box.cls)],
            "confidence": float(box.conf),
            "bbox": box.xyxy[0].tolist()
        })
    
    # Create annotated image
    annotated = results[0].plot()
    _, buffer = cv2.imencode('.jpg', annotated)
    image_base64 = base64.b64encode(buffer).decode()
    
    return {
        "objects": objects,
        "annotated_image": f"data:image/jpeg;base64,{image_base64}",
        "count": len(objects)
    }
```

**Detection Classes** (COCO dataset):
- Furniture: bed, chair, couch, dining table
- Decor: vase, potted plant, clock, book
- Electronics: tv, laptop, keyboard, mouse
- 80 total classes

### 3. Segment Service

**Technical Specs**:
- **Framework**: FastAPI + PyTorch + MobileSAM
- **Port**: 8002
- **Model**: MobileSAM (lightweight SAM)
- **Memory**: ~1.2 GB
- **CPU Usage**: 50-70% during inference
- **Response Time**: 2-5s (CPU), 0.8-1.2s (GPU)

**Segmentation Pipeline**:
```python
from segment_anything import sam_model_registry, SamPredictor
import torch

# Load MobileSAM
sam = sam_model_registry["vit_t"](checkpoint="app/mobile_sam.pt")
sam.eval()
predictor = SamPredictor(sam)

@app.post("/segment/")
async def segment_image(file: UploadFile, num_samples: int = 10):
    # Load image
    image = load_image(await file.read())
    
    # Set image for predictor
    predictor.set_image(image)
    
    # Generate point prompts (grid sampling)
    h, w = image.shape[:2]
    points = generate_grid_points(h, w, num_samples)
    
    # Generate masks
    masks, scores, _ = predictor.predict(
        point_coords=points,
        point_labels=np.ones(len(points)),
        multimask_output=False
    )
    
    # Create colored segmentation map
    segmented = create_colored_mask(image, masks)
    
    return {
        "segmented_image": encode_image(segmented),
        "num_regions": len(masks),
        "scores": scores.tolist()
    }
```

### 4. Advise Service

**Technical Specs**:
- **Framework**: FastAPI + Transformers
- **Port**: 8003
- **Model**: GPT-2 / Custom fine-tuned
- **Memory**: ~1.5 GB
- **CPU Usage**: 60-80% during generation
- **Response Time**: 3-8s (CPU), 1-2s (GPU)

**Advice Generation**:
```python
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer

# Load model
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

@app.post("/advise/")
async def get_design_advice(file: UploadFile, prompt: str = ""):
    # Analyze image (placeholder - would use vision model)
    image_features = await extract_features(file)
    
    # Generate context
    context = f"""Interior design advice for a {image_features['room_type']}:
    Current style: {image_features['style']}
    Colors detected: {image_features['colors']}
    User preference: {prompt}
    
    Professional recommendations:"""
    
    # Generate advice
    output = generator(
        context,
        max_length=200,
        num_return_sequences=1,
        temperature=0.7
    )
    
    advice = parse_advice(output[0]['generated_text'])
    
    return {
        "advice": advice,
        "room_type": image_features['room_type'],
        "style_suggestions": image_features['suggested_styles']
    }
```

### 5. Generate Service

**Technical Specs**:
- **Framework**: FastAPI + Diffusers
- **Port**: 8004
- **Model**: Stable Diffusion v1.5 + ControlNet
- **Memory**: ~4.5 GB
- **GPU VRAM**: 6-8 GB recommended
- **CPU Usage**: 80-100% during generation
- **Response Time**: 30-40s (CPU), 8-12s (GPU)

**Generation Pipeline**:
```python
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
import torch

# Load ControlNet
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-canny",
    torch_dtype=torch.float16
)

# Load Stable Diffusion pipeline
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    controlnet=controlnet,
    torch_dtype=torch.float16
)
pipe = pipe.to("cuda" if torch.cuda.is_available() else "cpu")

@app.post("/generate/")
async def generate_design(
    file: UploadFile,
    prompt: str,
    num_inference_steps: int = 20,
    guidance_scale: float = 7.5
):
    # Load and process image
    image = load_image(await file.read())
    
    # Generate Canny edges for ControlNet
    canny_image = generate_canny_edges(image)
    
    # Generate new design
    output = pipe(
        prompt=prompt,
        image=canny_image,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
    ).images[0]
    
    # Encode result
    buffered = BytesIO()
    output.save(buffered, format="JPEG", quality=95)
    img_str = base64.b64encode(buffered.getvalue()).decode()
    
    return {
        "generated_image": f"data:image/jpeg;base64,{img_str}",
        "prompt": prompt,
        "steps": num_inference_steps,
        "guidance": guidance_scale
    }
```

---

## API Reference

### Common Headers

All requests should include:
```http
Content-Type: application/json  # For JSON payloads
Content-Type: multipart/form-data  # For file uploads
```

### Authentication (Future)

```http
Authorization: Bearer eyJhbGciOiJIUzI1NiIs...
X-API-Key: your-api-key-here
```

### Rate Limiting (Planned)

```http
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 95
X-RateLimit-Reset: 1700000000
```

### Error Response Format

```json
{
  "error": {
    "code": "INVALID_IMAGE_FORMAT",
    "message": "Image must be JPEG or PNG",
    "details": {
      "received_format": "gif",
      "allowed_formats": ["jpeg", "png", "webp"]
    },
    "timestamp": "2025-11-22T10:30:00Z",
    "request_id": "req_abc123"
  }
}
```

### Service Endpoints

#### Detect Service (:8001)

**POST /detect/**
```bash
curl -X POST http://localhost:8001/detect/ \
  -F "file=@room.jpg"
```

Response:
```json
{
  "objects": [
    {
      "class": "bed",
      "confidence": 0.92,
      "bbox": [100, 150, 400, 350]
    }
  ],
  "annotated_image": "data:image/jpeg;base64,...",
  "count": 5,
  "processing_time": 2.3
}
```

#### Segment Service (:8002)

**POST /segment/**
```bash
curl -X POST http://localhost:8002/segment/ \
  -F "file=@room.jpg" \
  -F "num_samples=15"
```

#### Advise Service (:8003)

**POST /advise/**
```bash
curl -X POST http://localhost:8003/advise/ \
  -F "file=@room.jpg" \
  -F "prompt=modern minimalist"
```

#### Generate Service (:8004)

**POST /generate/**
```bash
curl -X POST http://localhost:8004/generate/ \
  -F "file=@room.jpg" \
  -F "prompt=modern bedroom with warm tones" \
  -F "num_inference_steps=20" \
  -F "guidance_scale=7.5"
```

---

## Database Design

### Collections

#### 1. designs

```javascript
{
  _id: ObjectId("6563a1b2c3d4e5f6g7h8i9j0"),
  user_id: "user_abc123",
  original_image_url: "s3://bucket/images/original_abc.jpg",
  workflow_results: {
    detection: {
      objects: [...],
      annotated_image_url: "s3://bucket/images/detect_abc.jpg",
      completed_at: ISODate("2025-11-22T10:30:15Z")
    },
    segmentation: {
      segmented_image_url: "s3://bucket/images/segment_abc.jpg",
      num_regions: 8,
      completed_at: ISODate("2025-11-22T10:30:20Z")
    },
    advice: {
      recommendations: [...],
      style_suggestions: [...],
      completed_at: ISODate("2025-11-22T10:30:25Z")
    },
    generation: {
      generated_image_url: "s3://bucket/images/generated_abc.jpg",
      prompt: "modern bedroom",
      parameters: {
        steps: 20,
        guidance: 7.5
      },
      completed_at: ISODate("2025-11-22T10:31:05Z")
    }
  },
  metadata: {
    total_processing_time: 45.2,
    workflow_status: "completed",
    error_count: 0
  },
  created_at: ISODate("2025-11-22T10:30:00Z"),
  updated_at: ISODate("2025-11-22T10:31:05Z")
}
```

#### Indexes

```javascript
// Performance indexes
db.designs.createIndex({ "user_id": 1, "created_at": -1 });
db.designs.createIndex({ "workflow_results.workflow_status": 1 });
db.designs.createIndex({ "created_at": -1 });
```

---

## Security & Authentication

### Current Implementation

**CORS Policy**:
```python
allow_origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173"
]
```

**File Upload Validation**:
```python
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10 MB
ALLOWED_EXTENSIONS = {".jpg", ".jpeg", ".png", ".webp"}

def validate_upload(file: UploadFile):
    # Check extension
    ext = Path(file.filename).suffix.lower()
    if ext not in ALLOWED_EXTENSIONS:
        raise HTTPException(400, "Invalid file type")
    
    # Check size
    file.file.seek(0, 2)
    size = file.file.tell()
    if size > MAX_FILE_SIZE:
        raise HTTPException(400, "File too large")
    
    file.file.seek(0)
```

### Production Security Checklist

- [ ] Implement JWT authentication
- [ ] Add API key validation
- [ ] Enable HTTPS/TLS
- [ ] Implement rate limiting (100 req/min)
- [ ] Add request signing
- [ ] Use environment variables for secrets
- [ ] Enable SQL/NoSQL injection protection
- [ ] Implement input sanitization
- [ ] Add CSRF protection
- [ ] Enable security headers (HSTS, CSP, etc.)
- [ ] Implement audit logging
- [ ] Add DDoS protection (Cloudflare)

---

## Performance & Optimization

### Benchmarks

**Hardware**: Intel i7-10700K, 32GB RAM, No GPU

| Workflow Step | Cold Start | Warm | P95 | P99 |
|--------------|-----------|------|-----|-----|
| Detect | 3.2s | 1.8s | 2.5s | 3.0s |
| Segment | 5.1s | 2.9s | 4.2s | 5.5s |
| Advise | 8.3s | 4.2s | 6.8s | 9.2s |
| Generate | 42.1s | 32.8s | 38.5s | 45.0s |
| **Total** | **58.7s** | **41.7s** | **52.0s** | **62.7s** |

**With GPU** (NVIDIA RTX 3060, 12GB VRAM):

| Workflow Step | Time | Speedup |
|--------------|------|---------|
| Detect | 0.4s | 4.5x |
| Segment | 0.9s | 3.2x |
| Advise | 1.8s | 2.3x |
| Generate | 9.2s | 3.6x |
| **Total** | **12.3s** | **3.4x** |

### Optimization Techniques

#### Backend Optimizations

**1. Model Caching**:
```python
# Load models once at startup, not per request
@app.on_event("startup")
async def load_models():
    global model
    model = YOLO("yolov8n.pt")
    model.fuse()  # Fuse layers for speed
```

**2. Batch Processing**:
```python
# Process multiple images in batch
@app.post("/detect/batch")
async def detect_batch(files: List[UploadFile]):
    images = [load_image(f) for f in files]
    results = model(images, batch=True)  # Batch inference
    return [process_result(r) for r in results]
```

**3. Response Compression**:
```python
from fastapi.middleware.gzip import GZipMiddleware
app.add_middleware(GZipMiddleware, minimum_size=1000)
```

**4. Connection Pooling**:
```python
client = AsyncIOMotorClient(
    MONGO_URI,
    maxPoolSize=100,
    minPoolSize=20
)
```

#### Frontend Optimizations

**1. Code Splitting**:
```javascript
const FullWorkflow = lazy(() => import('./pages/FullWorkflow'));
const Generate = lazy(() => import('./pages/Generate'));
```

**2. Image Optimization**:
```javascript
// Compress before upload
import imageCompression from 'browser-image-compression';

const compressed = await imageCompression(file, {
  maxSizeMB: 1,
  maxWidthOrHeight: 1920
});
```

**3. Caching Strategy**:
```javascript
// Service Worker caching
self.addEventListener('fetch', (event) => {
  event.respondWith(
    caches.match(event.request)
      .then(response => response || fetch(event.request))
  );
});
```

---

## Deployment

### Docker Deployment

**docker-compose.yml**:
```yaml
version: '3.8'

services:
  gateway:
    build: ./artistry-backend/gateway
    ports:
      - "8000:8000"
    environment:
      - MONGO_URI=${MONGO_URI}
    restart: unless-stopped

  detect:
    build: ./artistry-backend/detect
    ports:
      - "8001:8001"
    volumes:
      - ./models/yolov8n.pt:/app/yolov8n.pt
    restart: unless-stopped

  segment:
    build: ./artistry-backend/segment
    ports:
      - "8002:8002"
    volumes:
      - ./models/mobile_sam.pt:/app/mobile_sam.pt
    restart: unless-stopped

  advise:
    build: ./artistry-backend/advise
    ports:
      - "8003:8003"
    restart: unless-stopped

  generate:
    build: ./artistry-backend/generate
    ports:
      - "8004:8004"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  frontend:
    build: ./frontend
    ports:
      - "80:80"
    depends_on:
      - gateway
    restart: unless-stopped
```

### Cloud Deployment

**AWS Architecture**:
```
Route 53 (DNS)
    â†“
CloudFront (CDN) â†’ S3 (Frontend static files)
    â†“
ALB (Load Balancer)
    â†“
ECS Fargate (Containers)
â”œâ”€â”€ Gateway Service (t3.medium)
â”œâ”€â”€ Detect Service (t3.large)
â”œâ”€â”€ Segment Service (t3.large)
â”œâ”€â”€ Advise Service (g4dn.xlarge - GPU)
â””â”€â”€ Generate Service (g4dn.2xlarge - GPU)
    â†“
MongoDB Atlas (us-east-1)
```

**Estimated Costs** (AWS):
- Frontend (S3 + CloudFront): $5-10/month
- Backend Services (ECS): $150-300/month
- GPU Instances: $200-400/month
- MongoDB Atlas: Free tier
- **Total**: ~$355-710/month

---

## Development Guide

### Setting Up Development Environment

```bash
# Backend
cd artistry-backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install pytest black flake8  # Dev tools

# Frontend
cd frontend
npm install
npm install -D @testing-library/react vitest  # Dev tools
```

### Code Quality Tools

**Backend**:
```bash
# Format code
black artistry-backend/

# Lint
flake8 artistry-backend/ --max-line-length=100

# Type checking
mypy artistry-backend/
```

**Frontend**:
```bash
# Lint
npm run lint

# Format
npm run format

# Type check
npm run type-check
```

### Adding a New Service

1. Create service directory
2. Create Dockerfile
3. Implement FastAPI app
4. Add to docker-compose.yml
5. Update gateway routing
6. Add frontend API function
7. Write tests
8. Update documentation

---

## Testing Strategy

### Backend Tests

```python
# tests/test_detect.py
import pytest
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

def test_detect_endpoint():
    with open("test_image.jpg", "rb") as f:
        response = client.post(
            "/detect/",
            files={"file": ("test.jpg", f, "image/jpeg")}
        )
    assert response.status_code == 200
    assert "objects" in response.json()
```

### Frontend Tests

```javascript
// tests/FullWorkflow.test.jsx
import { render, screen, fireEvent } from '@testing-library/react';
import FullWorkflow from '../src/pages/FullWorkflow';

test('uploads image and starts workflow', async () => {
  render(<FullWorkflow />);
  const file = new File(['content'], 'test.jpg', { type: 'image/jpeg' });
  const input = screen.getByLabelText(/upload/i);
  
  fireEvent.change(input, { target: { files: [file] } });
  fireEvent.click(screen.getByText(/start workflow/i));
  
  expect(await screen.findByText(/detecting/i)).toBeInTheDocument();
});
```

---

## Troubleshooting

See [Troubleshooting Guide](./troubleshooting/COMMON_ISSUES.md) for detailed solutions.

---

## Future Enhancements

### Phase 1 (Q1 2026)
- JWT authentication
- User profiles and history
- Design saving/loading
- Before/after slider

### Phase 2 (Q2 2026)
- WebSocket for real-time updates
- Batch processing
- 3D visualization
- Mobile app (React Native)

### Phase 3 (2027)
- AR preview
- VR walkthrough
- Multi-room projects
- Professional marketplace

---

**For More Information**:
- [System Architecture](./architecture/SYSTEM_ARCHITECTURE.md)
- [API Documentation](./api/)
- [Installation Guide](./guides/INSTALLATION.md)

---

**Version**: 2.0  
**Last Updated**: November 22, 2025  
**Maintainer**: Kushagra Bhardwaj
